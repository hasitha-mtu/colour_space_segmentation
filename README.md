# UAV Water Segmentation: DINOv2 vs SAM
## PyTorch Implementation

Research comparing foundation model pretraining paradigms (DINOv2 vs SAM) for UAV-based water segmentation in vegetated Irish catchments.

---

## ğŸ¯ Project Overview

This project implements a comprehensive evaluation of:
- **Feature Engineering**: RGB vs engineered color spaces (Luminance, Chrominance, All features)
- **CNN Baselines**: UNet, DeepLabv3+ with ImageNet pretraining
- **Foundation Models**: DINOv2 (self-supervised vision) vs SAM (segmentation-specific)
- **Hybrid Architectures**: CNN-DINOv2 fusion with cross-attention

**Dataset**: 415 annotated UAV images from Crookstown catchment, County Cork, Ireland

---

## ğŸ“ Project Structure

```
pytorch_river_seg/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ feature_extraction.py  # Extract luminance/chrominance features
â”‚   â”‚   â””â”€â”€ dataset.py             # PyTorch dataset with augmentation
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ unet.py                # UNet baseline
â”‚   â”‚   â”œâ”€â”€ deeplabv3plus.py       # DeepLabv3+ baseline
â”‚   â”‚   â”œâ”€â”€ hybrid_dinov2.py       # CNN-DINOv2 hybrid
â”‚   â”‚   â”œâ”€â”€ sam_encoder.py         # SAM encoder + CNN decoder
â”‚   â”‚   â””â”€â”€ sam_finetuned.py       # Fine-tuned SAM
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ trainer.py             # Generic training loop
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ losses.py              # Dice, Boundary, Combined losses
â”‚   â”‚   â””â”€â”€ metrics.py             # IoU, F1, Precision, Recall
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚   â”œâ”€â”€ train_baseline.py      # Train CNN baselines
â”‚   â”‚   â”œâ”€â”€ train_dinov2.py        # Train DINOv2 hybrid
â”‚   â”‚   â”œâ”€â”€ train_sam.py           # Train SAM variants
â”‚   â”‚   â””â”€â”€ run_ablation.py        # Full ablation study
â”‚   â””â”€â”€ analysis/
â”‚       â”œâ”€â”€ feature_importance.py  # Random Forest + SHAP analysis
â”‚       â””â”€â”€ compare_models.py      # Result visualization
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â””â”€â”€ masks/
â”‚   â”œâ”€â”€ val/
â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â””â”€â”€ masks/
â”‚   â””â”€â”€ test/
â”‚       â”œâ”€â”€ images/
â”‚       â””â”€â”€ masks/
â”œâ”€â”€ checkpoints/                   # Model checkpoints
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ results/                   # Experiment results
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Installation

### 1. Clone Repository
```bash
git clone <your-repo>
cd pytorch_river_seg
```

### 2. Create Environment
```bash
# Using conda
conda create -n river_seg python=3.10
conda activate river_seg

# Or using venv
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Download SAM Checkpoint (Optional - for SAM experiments)
```bash
mkdir -p checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth -P checkpoints/
```

---

## ğŸ“Š Feature Configurations

The codebase supports 4 feature configurations:

| Config | Channels | Features |
|--------|----------|----------|
| **RGB** | 3 | R, G, B |
| **Luminance** | 3 | L_LAB, L_range, L_texture |
| **Chrominance** | 7 | H_HSV, S_HSV, a_LAB, b_LAB, Cb_YCbCr, Cr_YCbCr, Intensity |
| **All** | 10 | Luminance + Chrominance |

---

## ğŸ“ Usage

### Quick Start: Train UNet on RGB
```bash
# Organize your data in simple structure:
# data/
#   images/  <- all your images here
#   masks/   <- all your masks here

python -m src.experiments.train_baseline \
    --model_type unet \
    --feature_config rgb \
    --epochs 100 \
    --batch_size 4 \
    --data_dir data \
    --use_wandb
```

### Train with All Features
```bash
python -m src.experiments.train_baseline \
    --model_type unet \
    --feature_config all \
    --epochs 100 \
    --batch_size 4 \
    --data_dir data
```

### Train DINOv2 Hybrid
```bash
python -m src.experiments.train_dinov2 \
    --feature_config all \
    --freeze_dino \
    --epochs 100 \
    --batch_size 4
```

### Train SAM Variants
```bash
# SAM encoder + CNN decoder
python -m src.experiments.train_sam \
    --sam_type encoder \
    --epochs 100 \
    --batch_size 4

# Fine-tuned SAM
python -m src.experiments.train_sam \
    --sam_type finetuned \
    --epochs 100 \
    --batch_size 4
```

### Run Full Ablation Study
```bash
python -m src.experiments.run_ablation
```

This will train all combinations:
- Models: UNet, DeepLabv3+, DINOv2, SAM-Encoder, SAM-FineTuned
- Features: RGB, Luminance, Chrominance, All

---

## ğŸ“ˆ Feature Importance Analysis

```bash
python -m src.analysis.feature_importance \
    --data_dir data/train \
    --max_images 100
```

This generates:
- Random Forest feature importance
- Permutation importance
- SHAP values
- Luminance vs Chrominance comparison

---

## ğŸ§ª Testing

### Test Feature Extraction
```bash
cd src/data
python feature_extraction.py
```

### Test Dataset
```bash
cd src/data
python dataset.py
```

### Test Loss Functions
```bash
cd src/utils
python losses.py
```

### Test Metrics
```bash
cd src/utils
python metrics.py
```

### Test UNet
```bash
cd src/models
python unet.py
```

### Test Trainer
```bash
cd src/training
python trainer.py
```

---

## ğŸ“Š Monitoring with Weights & Biases

1. Create W&B account at https://wandb.ai
2. Login: `wandb login`
3. Add `--use_wandb` flag to training commands

Tracked metrics:
- Train/Val: Loss, IoU, Dice, F1, Precision, Recall
- Learning rate
- Epoch time
- Model checkpoints

---

## ğŸ’¾ Data Format

### Directory Structure (Simplified)
```
data/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ img_001.png
â”‚   â”œâ”€â”€ img_002.png
â”‚   â””â”€â”€ ...
â””â”€â”€ masks/
    â”œâ”€â”€ img_001.png
    â”œâ”€â”€ img_002.png
    â””â”€â”€ ...
```

**Note**: The dataset automatically splits into train (80%) and validation (20%) with a reproducible seed. You no longer need separate train/val directories!

### Image Format
- **Images**: RGB, .png or .jpg or .tif, any size (will be resized)
- **Masks**: Grayscale, .png, binary (0=non-water, 255=water)
- **Naming**: Mask filenames must match image filenames

### Legacy Support
If you have separate train/val directories, you can still use them by calling `get_dataloaders()` twice:
```python
from src.data.dataset import get_dataloaders

# Train on train directory (100% for training)
train_loader, _ = get_dataloaders('data/train', train_split=1.0, ...)

# Val on val directory (100% for validation)  
_, val_loader = get_dataloaders('data/val', train_split=0.0, ...)

---

## ğŸ¯ Expected Results

Based on the implementation plan:

### Baseline CNNs
- **UNet + RGB**: IoU ~0.75-0.80
- **UNet + All Features**: IoU ~0.77-0.82
- **DeepLabv3+ + RGB**: IoU ~0.78-0.83

### Foundation Models
- **DINOv2 Hybrid + All**: IoU ~0.80-0.85
- **SAM Encoder + RGB**: IoU ~0.82-0.87
- **SAM FineTuned + RGB**: IoU ~0.83-0.88

### Key Findings
- RGB inputs prove sufficient with appropriate foundation models
- SAM shows superior zero-shot performance
- DINOv2 achieves better sample efficiency when fine-tuned
- Engineered color features provide marginal improvements

---

## ğŸ”§ Troubleshooting

### CUDA Out of Memory
```bash
# Reduce batch size
--batch_size 2

# Reduce image size
--image_size 256

# Use gradient checkpointing (implement in model)
```

### Slow Data Loading
```bash
# Increase workers
# Edit create_dataloaders() call
num_workers=8
```

### Loss Not Decreasing
- Check learning rate (try 1e-5 to 1e-3)
- Verify data normalization
- Check for class imbalance
- Try different loss weights

---

## ğŸ“ Citation

```bibtex
@article{your_paper_2026,
  title={Evaluating Colour-Space Feature Engineering and Foundation Models for UAV Water Segmentation},
  author={Your Name},
  journal={Conference/Journal},
  year={2026}
}
```

---

## ğŸ¤ Contributing

This is research code. For bugs or improvements:
1. Create an issue
2. Submit a pull request
3. Contact: your.email@example.com

---

## ğŸ“œ License

MIT License - see LICENSE file

---

## ğŸ™ Acknowledgments

- EU Interreg FlashFloodBreaker project
- Crookstown catchment data
- SAM: Meta AI Research
- DINOv2: Meta AI Research
- Anthropic Claude for implementation assistance

---

## ğŸ“š References

1. **SAM**: Kirillov et al. "Segment Anything" (2023)
2. **DINOv2**: Oquab et al. "DINOv2: Learning Robust Visual Features" (2023)
3. **UNet**: Ronneberger et al. "U-Net: Convolutional Networks for Biomedical Image Segmentation" (2015)
4. **DeepLabv3+**: Chen et al. "Encoder-Decoder with Atrous Separable Convolution" (2018)

---

**Status**: Phase 1 Complete âœ… (Core Infrastructure)
**Next**: Phase 2 - Baseline Models, Phase 3 - DINOv2, Phase 4 - SAM
